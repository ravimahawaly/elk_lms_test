Filebeat installation steps in Redhat
=================================================
#	sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch
#	vim /etc/yum.repos.d/elastic.repo
	[elastic-7.x]
	name=Elastic repository for 7.x packages
	baseurl=https://artifacts.elastic.co/packages/9.x/yum
	gpgcheck=1
	gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
	enabled=1
	autorefresh=1
	type=rpm-md
	:wq!
#	yum clean all
#	yum repolist all
#	yum list all | grep filebeat
#	sudo yum install filebeat
#	cd /etc/filebeat/certs/ca
#	mkdir certs/   
#	cd certs/
#	ls 
	==> Here you will see 3 certs : ca.crt, filebeat.crt, filebeat.key
		ca.crt ==> ca signed and must match logstash (kt-ca.crt) => you can copy contents from logstash(kt-ca.crt) and paste here under ca.crt
		filebeat.key => I have copied the contents from BIS-Application and paste here
		filebeat.crt => I have copied the contents from BIS-Application and paste here  ==> self-signed
#	cd ..
#	vim /etc/filebeat/filebeat.yml
	filebeat.inputs:
	- type: log
	  id: dos_canvas_test_application
	  enabled: true
	  paths:
        - /dos-canvas/logs/app.log
	  tags: ["dos_canvas_test_application"]              ==> will tag every log with name "dev-api-logs"

	- type: log
      id: dos_canvas_uat_application
      enabled: true
      paths:
        - /dos-canvas/dos_prod_logs
      tags: ["dos_canvas_uat_application"]
	# ------------------------------ Logstash Output -------------------------------
	output.logstash:
	  hosts: ["awsireelk02.kuonitumlare.int:5044","awsireelk03.kuonitumlare.int:5044"]
	  loadbalance: true
	  ssl.certificate_authorities: ["/etc/filebeat/certs/ca.crt"]
	  ssl.certificate: "/etc/filebeat/certs/filebeat.crt"
	  ssl.verification_mode: none
	:wq!
	
	======> Now before starting filebeat service, make changes to the logstash.conf file and restart logstash container==============
#	sudo systemctl enable filebeat
#	sudo systemctl start filebeat
#	systemctl status filebeat.service
#	journalctl -u filebeat.service -f
=============================================================================================
Logstash Changes:
=============================================================================================
#	vim logstash.conf
	input {
			tcp {
					port => 5000
					codec => "json_lines"
			}
			beats {
			port => 5044
        	ssl => true
        	ssl_certificate_authorities => ["/usr/share/logstash/config/ca.crt"]
        	ssl_certificate => "/usr/share/logstash/config/logstash-input.crt"
        	ssl_key => "/usr/share/logstash/config/logstash-input.key"
        	ssl_verify_mode => "force_peer"
			}
	}
	
	filter {
	if "dos_canvas_test_application" in [tags] {                 ==> filter logs using dev-api-logs
	grok {
	  match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:thread}\] %{LOGLEVEL:loglevel}  %{JAVACLASS:class} - %{GREEDYDATA:log_message}" }
	}
	date {
        match => ["timestamp", "YYYY-MM-dd HH:mm:ss"]
        timezone => "UTC"
		}
      mutate {
        remove_field => ["timestamp"]
      }
	}
	}
	filter {
	if "dos_canvas_uat_application" in [tags] {
	grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:thread}\] %{LOGLEVEL:loglevel} %{JAVACLASS:class} - %{GREEDYDATA:log_message}" }
	}
    date {
        match => ["timestamp", "YYYY-MM-dd HH:mm:ss"]
        timezone => "UTC"
      }
      mutate {
        remove_field => ["timestamp"]
      }

	}
	}

	output {
	if "dos_canvas_test_application" in [tags] {
	  elasticsearch {
		hosts => ["https://awsireelk02.kuonitumlare.int","https://awsireelk03.kuonitumlare.int"]
		index => "dos-canvas-test-app"            ==> This is the name which will appear at kibana for indexing
		acert => "/usr/share/logstash/config/kt-ca.crt"
		keystore => "/usr/share/logstash/config/logstash.p12"
		keystore_password => ""
		ssl_certificate_verification => false
		user => "usrname"
		password => "password"
	#ilm_rollover_alias => "flight-search"
	#ilm_pattern => "000001"
	}
	  stdout { codec => rubydebug }
	}}
	
	output {
	if "dos_canvas_uat_application" in [tags] {
	  elasticsearch {
		hosts => ["https://awsireelk02.kuonitumlare.int","https://awsireelk03.kuonitumlare.int"]
		index => "dos-canvas-uat-app"            ==> This is the name which will appear at kibana for indexing
		acert => "/usr/share/logstash/config/kt-ca.crt"
		keystore => "/usr/share/logstash/config/logstash.p12"
		keystore_password => ""
		ssl_certificate_verification => false
		user => "usrname"
		password => "password"
	#ilm_rollover_alias => "flight-search"
	#ilm_pattern => "000001"
	}
	  stdout { codec => rubydebug }
	}}
	:wq!
#	cat logstash.conf
#	docker restart logstash           ==> If running on single node docker
#	docker logs logstash -f           ==> Now enable/start/restart filebeat.service

================================================================================================
How to deploy logstash on Docker Swarm Cluster
================================================================================================

#	cd /docker/deployments/elk_mig/logstash/pipeline/
#	ls
#	mv logstash.conf logstash.conf_11092025
#	cp -r /home/elkadmin/ravi/logstash.conf ./
#	chmod +x logstash.conf
#	ls
#	cd /docker/deployments/elk_mig/
#	docker stack ls
#	docker stack rm logstash
#	docker stack deploy --compose-file docker-logstash.yml --with-registry-auth logstash
#	docker stack ls
#	docker logs logstash -f  ==> to check logs input from filebeat  => do this before starting filebeat.service

===========> Now after deploying logstash, start the filebeat service.

==================================================================================================
#	logstash.conf

input {
        tcp {
                port => 5000
                codec => "json_lines"
        }
        beats {
        port => 5044
        ssl => true
        ssl_certificate_authorities => ["/usr/share/logstash/config/ca.crt"]
        ssl_certificate => "/usr/share/logstash/config/logstash-input.crt"
        ssl_key => "/usr/share/logstash/config/logstash-input.key"
        ssl_verify_mode => "force_peer"
        }
}

filter {
if "dos_canvas_uat_application" in [tags] {
  grok {
    match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:thread}\] %{LOGLEVEL:loglevel} %{JAVACLASS:class} - %{GREEDYDATA:log_message}" }
}
  date {
      match => ["timestamp", "YYYY-MM-dd HH:mm:ss"]
      timezone => "UTC"
    }
    mutate {
      remove_field => ["timestamp"]
    }

}
}

filter {
if "dos_canvas_prod_application" in [tags] {
  grok {
    match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \\[%{DATA:thread}\\] %{LOGLEVEL:loglevel} +%{JAVACLASS:class} - %{GREEDYDATA:message}" }
}
  date {
      match => ["timestamp", "YYYY-MM-dd HH:mm:ss"]
      timezone => "UTC"
    }
    mutate {
      remove_field => ["timestamp"]
    }

}
}


output {
if "dos_canvas_uat_application" in [tags] {
  elasticsearch {
    hosts => ["https://awsireelk02.kuonitumlare.int","https://awsireelk03.kuonitumlare.int"]
    index => "dos-canvas-uat-app"
    cacert => "/usr/share/logstash/config/kt-ca.crt"
    keystore => "/usr/share/logstash/config/logstash.p12"
    keystore_password => ""
    ssl_certificate_verification => false
    user => "elastic"
    password => "o^Ytt9%GSzEO"
#ilm_rollover_alias => "dos-canvas-uat-app"
#ilm_pattern => "000001"
  }
  stdout { codec => rubydebug }
}}

output {
if "dos_canvas_prod_application" in [tags] {
  elasticsearch {
    hosts => ["https://awsireelk02.kuonitumlare.int","https://awsireelk03.kuonitumlare.int"]
    index => "dos-canvas-prod-app"
    cacert => "/usr/share/logstash/config/kt-ca.crt"
    keystore => "/usr/share/logstash/config/logstash.p12"
    keystore_password => ""
    ssl_certificate_verification => false
    user => "elastic"
    password => "o^Ytt9%GSzEO"

#ilm_rollover_alias => "flight-search"
#ilm_pattern => "000001"
  }
  stdout { codec => rubydebug }
}}

=========================================================================================
# cat filebeat.yml  ==> for reference

[root@DNKCPHAPI902 filebeat]# cat filebeat.yml
###################### Filebeat Configuration Example #########################

# This file is an example configuration file highlighting only the most common
# options. The filebeat.reference.yml file from the same directory contains all the
# supported options with more comments. You can use it as a reference.
#
# You can find the full configuration reference here:
# https://www.elastic.co/guide/en/beats/filebeat/index.html

# For more available modules and options, please see the filebeat.reference.yml sample
# configuration file.

# ============================== Filebeat inputs ===============================

filebeat.inputs:

# Each - is an input. Most options can be set at the input level, so
# you can use different inputs for various configurations.
# Below are the input specific configurations.

# filestream is an input for collecting log messages from files.
- type: log

  # Unique ID among all inputs, an ID is required.
  id: dos_canvas_uat_application

  # Change to true to enable this input configuration.
  enabled: true

  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /dos-canvas/logs/app.log
  tags: ["dos_canvas_uat_application"]
    #- c:\programdata\elasticsearch\logs\*

  # Exclude lines. A list of regular expressions to match. It drops the lines that are
  # matching any regular expression from the list.
  #exclude_lines: ['^DBG']

  # Include lines. A list of regular expressions to match. It exports the lines that are
  # matching any regular expression from the list.
  #include_lines: ['^ERR', '^WARN']

  # Exclude files. A list of regular expressions to match. Filebeat drops the files that
  # are matching any regular expression from the list. By default, no files are dropped.
  #prospector.scanner.exclude_files: ['.gz$']

  # Optional additional fields. These fields can be freely picked
  # to add additional information to the crawled log files for filtering
  #fields:
  #  level: debug
  #  review: 1

# ============================== Filebeat modules ==============================

filebeat.config.modules:
  # Glob pattern for configuration loading
  path: ${path.config}/modules.d/*.yml

  # Set to true to enable config reloading
  reload.enabled: false

  # Period on which files under path should be checked for changes
  #reload.period: 10s

# ======================= Elasticsearch template setting =======================

setup.template.settings:
  index.number_of_shards: 1
  #index.codec: best_compression
  #_source.enabled: false


# ================================== General ===================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
#name:

# The tags of the shipper are included in their own field with each
# transaction published.
#tags: ["service-X", "web-tier"]

# Optional fields that you can specify to add additional information to the
# output.
#fields:
#  env: staging

# ================================= Dashboards =================================
# These settings control loading the sample dashboards to the Kibana index. Loading
# the dashboards is disabled by default and can be enabled either by setting the
# options here or by using the `setup` command.
#setup.dashboards.enabled: false

# The URL from where to download the dashboards archive. By default this URL
# has a value which is computed based on the Beat name and version. For released
# versions, this URL points to the dashboard archive on the artifacts.elastic.co
# website.
#setup.dashboards.url:

# =================================== Kibana ===================================

# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.
# This requires a Kibana endpoint configuration.
setup.kibana:

  # Kibana Host
  # Scheme and port can be left out and will be set to the default (http and 5601)
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601
  #host: "localhost:5601"

  # Kibana Space ID
  # ID of the Kibana Space into which the dashboards should be loaded. By default,
  # the Default Space will be used.
  #space.id:

# =============================== Elastic Cloud ================================

# These settings simplify using Filebeat with the Elastic Cloud (https://cloud.elastic.co/).

# The cloud.id setting overwrites the `output.elasticsearch.hosts` and
# `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
#cloud.id:

# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `<user>:<pass>`.
#cloud.auth:

# ================================== Outputs ===================================

# Configure what output to use when sending the data collected by the beat.

# ---------------------------- Elasticsearch Output ----------------------------
#output.elasticsearch:
  # Array of hosts to connect to.
  # hosts: ["localhost:9200"]

  # Protocol - either `http` (default) or `https`.
  #protocol: "https"

  # Authentication credentials - either API key or username/password.
  #api_key: "id:api_key"
  #username: "elastic"
  #password: "changeme"

# ------------------------------ Logstash Output -------------------------------
#output.logstash:
  # The Logstash hosts
  #hosts: ["localhost:5044"]
output.logstash:
  # The Logstash hosts
  #hosts: ["10.15.1.170:5044"]
  hosts: ["awsireelk02.kuonitumlare.int:5044","awsireelk03.kuonitumlare.int:5044"]
  loadbalance: true
  ssl.certificate_authorities: ["/etc/filebeat/certs/ca.crt"]
  ssl.certificate: "/etc/filebeat/certs/filebeat.crt"
  ssl.key: "/etc/filebeat/certs/filebeat.key"
  ssl.verification_mode: none
  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

# ================================= Processors =================================
processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
  - add_kubernetes_metadata: ~

# ================================== Logging ===================================

# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
#logging.level: debug

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors use ["*"]. Examples of other selectors are "beat",
# "publisher", "service".
#logging.selectors: ["*"]

# ============================= X-Pack Monitoring ==============================
# Filebeat can export internal metrics to a central Elasticsearch monitoring
# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The
# reporting is disabled by default.

# Set to true to enable the monitoring reporter.
#monitoring.enabled: false

# Sets the UUID of the Elasticsearch cluster under which monitoring data for this
# Filebeat instance will appear in the Stack Monitoring UI. If output.elasticsearch
# is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch.
#monitoring.cluster_uuid:

# Uncomment to send the metrics to Elasticsearch. Most settings from the
# Elasticsearch output are accepted here as well.
# Note that the settings should point to your Elasticsearch *monitoring* cluster.
# Any setting that is not set is automatically inherited from the Elasticsearch
# output configuration, so if you have the Elasticsearch output configured such
# that it is pointing to your Elasticsearch monitoring cluster, you can simply
# uncomment the following line.
#monitoring.elasticsearch:

# ============================== Instrumentation ===============================

# Instrumentation support for the filebeat.
#instrumentation:
    # Set to true to enable instrumentation of filebeat.
    #enabled: false

    # Environment in which filebeat is running on (eg: staging, production, etc.)
    #environment: ""

    # APM Server hosts to report instrumentation results to.
    #hosts:
    #  - http://localhost:8200

    # API Key for the APM Server(s).
    # If api_key is set then secret_token will be ignored.
    #api_key:

    # Secret token for the APM Server(s).
    #secret_token:


# ================================= Migration ==================================

# This allows to enable 6.7 migration aliases
#migration.6_to_7.enabled: true


